{"cells":[{"cell_type":"markdown","metadata":{"id":"RBew83r9kjHs"},"source":["## Reidintification Notebook\n","### Deep Learning project - Task #1\n","*Wamiq Raza*"]},{"cell_type":"markdown","metadata":{"id":"I2JtBIEtkvTG"},"source":["### Setup\n","\n","Import section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDYIw6lh-e7O"},"outputs":[],"source":["import pandas as pd  \n","import numpy as np   \n","import csv \n","\n","import torch\n","from torch import nn, optim\n","import torchvision\n","from torchvision import transforms, datasets, models, utils\n","from torch.utils.data import Dataset, DataLoader \n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from torch.nn import functional as F\n","from skimage import io, transform\n","from sklearn.metrics import pairwise_distances\n","from torch.optim import lr_scheduler\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","import os\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"markdown","metadata":{"id":"S7NPq3iHlEbw"},"source":["Global variables definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gu3Z36-GE9SQ"},"outputs":[],"source":["DATASET_PATH = '/content/dataset/'\n","MODEL_PATH = '/content/reid_model.pth'\n","\n","TRAIN_FOLDER_PATH = DATASET_PATH + 'train'\n","ANNOTATION_CSV_PATH = DATASET_PATH + 'annotations_train.csv'\n","\n","QUERY_CSV_PATH = DATASET_PATH + 'queries.csv'\n","QUERY_FOLDER_PATH = DATASET_PATH + 'queries/'\n","TEST_CSV_PATH = DATASET_PATH + 'tests.csv'\n","TEST_FOLDER_PATH = DATASET_PATH + 'test/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lW8CQ3xqlNDg"},"outputs":[],"source":["TRAIN_PERCENTAGE = 80\n","BATCH_SIZE = 64\n","EPOCHS = 100"]},{"cell_type":"markdown","metadata":{"id":"_mqtV-r3lWFJ"},"source":["Device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A84dSeTVnVoN"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() \n","                                  else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"EJnI8pj9lZ4p"},"source":["## Dataset\n","\n","Import the dataset and extract the zip file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvUzRuRpJ2zr"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","!unzip \"/content/drive/MyDrive/dataset.zip\" -d dataset"]},{"cell_type":"markdown","metadata":{"id":"xNfsp8KMlxzK"},"source":["### Early Stopping\n","\n","patience of 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzDQ_BLvdXX-"},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path=MODEL_PATH, trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}, {type(self.counter)}, {type(self.patience)}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model, self.path)\n","        self.val_loss_min = val_loss\n"]},{"cell_type":"markdown","metadata":{"id":"5uvljQjol5FU"},"source":["## mAP computation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N826gmCdmbSa"},"outputs":[],"source":["from typing import Dict, Set, List\n","\n","\n","class Evaluator:\n","\n","    @staticmethod\n","    def evaluate_map(predictions: Dict[str, List], ground_truth: Dict[str, Set]):\n","        '''\n","        Computes the mAP (https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173) of the predictions with respect to the given ground truth\n","        In person reidentification mAP refers to the mean of the AP over all queries.\n","        The AP for a query is the area under the precision-recall curve obtained from the list of predictions considering the\n","        ground truth elements as positives and the other ones as negatives\n","\n","        :param predictions: dictionary from query filename to list of test image filenames associated with the query ordered\n","                            from the most to the least confident prediction.\n","                            Represents the predictions to be evaluated.\n","        :param ground_truth: dictionary from query filename to set of test image filenames associated with the query\n","                             Represents the ground truth on which to evaluate predictions.\n","\n","        :return:\n","        '''\n","\n","        m_ap = 0.0\n","        for current_ground_truth_query, current_ground_truth_query_set in ground_truth.items():\n","\n","            # No predictions were performed for the current query, AP = 0\n","            if not current_ground_truth_query in predictions:\n","                continue\n","\n","            current_ap = 0.0  # The area under the curve for the current sample\n","            current_predictions_list = predictions[current_ground_truth_query]\n","\n","            # Recall increments of this quantity each time a new correct prediction is encountered in the prediction list\n","            delta_recall = 1.0 / len(current_ground_truth_query_set)\n","\n","            # Goes through the list of predictions\n","            encountered_positives = 0\n","            for idx, current_prediction in enumerate(current_predictions_list):\n","                # Each time a positive is encountered, compute the current precition and the area under the curve\n","                # since the last positive\n","                if current_prediction in current_ground_truth_query_set:\n","                    encountered_positives += 1\n","                    current_precision = encountered_positives / (idx + 1)\n","                    current_ap += current_precision * delta_recall\n","\n","            m_ap += current_ap\n","\n","        # Compute mean over all queries\n","        m_ap /= len(ground_truth)\n","\n","        return m_ap"]},{"cell_type":"markdown","metadata":{"id":"b1qOdIYaxrCM"},"source":["Preprocessing (create one new csv file without the attributes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgEEGmWUnNxQ"},"outputs":[],"source":["def new_annotation(tr_path, ann_path):\n","  # read csv\n","  ids = pd.read_csv(ann_path)\n","  train_imgs = os.listdir(tr_path)\n","\n","  # crate a dataframe\n","  train_imgs_data = []\n","  for s in train_imgs:\n","    train_imgs_data.append([s, int(s.split('_')[0])])\n","  #print(train_imgs_data)\n","\n","  train_imgs_df = pd.DataFrame(train_imgs_data, columns=['path', 'id'])\n","\n","  train_imgs_df=train_imgs_df.sort_values(\"id\")\n","  train_imgs_df.reset_index(inplace=True)\n","  train_imgs_df.drop(\"index\",axis=1,inplace=True)\n","  \n","  # Encode target labels with value between 0 and n_classes-1.\n","  label_encoder = preprocessing.LabelEncoder()\n","  train_imgs_df['id']= label_encoder.fit_transform(train_imgs_df['id'])\n","  annotation_reid_path = DATASET_PATH + 'annotations_reid.csv'\n","  train_imgs_df.to_csv(annotation_reid_path)\n","  train_imgs_df.head()\n","\n","\n","  return annotation_reid_path\n","\n","annotation_reid_path= new_annotation(TRAIN_FOLDER_PATH, ANNOTATION_CSV_PATH)"]},{"cell_type":"markdown","metadata":{"id":"WI-i1kh9o_n5"},"source":["Train dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yY8b7Q8GTSoT"},"outputs":[],"source":["from torchvision.io import read_image\n","class custom_dataset(Dataset):\n","    def __init__(self, csv_file, root_dir, transform):\n","        self.annotations = pd.read_csv(csv_file,index_col=[0])\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir,str(self.annotations.iloc[index, 0]))\n","        image = read_image(img_path)\n","        image=image.squeeze().permute(1,2,0)\n","        label_id=np.array([self.annotations.iloc[index,1]]).astype('int')\n","\n","        anchor_img = image\n","        anchor_label = int(label_id)   \n","\n","        positive_list = self.annotations[self.annotations[\"id\"]==anchor_label].index\n","        positive_item = random.choice(positive_list)\n","        positive_img = self.annotations.path[positive_item]\n","        positive_img_path=os.path.join(self.root_dir,positive_img)\n","        positive_img=read_image(positive_img_path)\n","\n","\n","        negative_list = self.annotations[self.annotations[\"id\"]!=anchor_label].index\n","        negative_item = random.choice(negative_list)\n","        negative_img = self.annotations.path[negative_item]\n","        negative_img_path=os.path.join(self.root_dir,negative_img)\n","        negative_img=read_image(negative_img_path)\n","        sample={\"anchor_image\":np.uint8(image),\"img_path\":img_path,\"positive_image\":positive_img,\"negative_image\":negative_img,'label_id': label_id}\n","\n","        if self.transform:\n","            sample[\"anchor_image\"] = self.transform(sample[\"anchor_image\"])\n","            sample[\"positive_image\"] = self.transform(sample[\"positive_image\"])\n","            sample[\"negative_image\"] = self.transform(sample[\"negative_image\"])\n","       \n","        return (sample)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vRC8JYJomTNO"},"source":["Test dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsC2zHNypFeb"},"outputs":[],"source":["class custom_dataset_test(Dataset):\n","    def __init__(self, csv_file, root_dir, transform):\n","        self.annotations = pd.read_csv(csv_file,index_col=[0])\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir,str(self.annotations.iloc[index, 0]))\n","        image = read_image(img_path)\n","        image=image.squeeze().permute(1,2,0)\n","\n","        sample={\"image\":np.uint8(image),\"image_path\":img_path}\n","\n","        if self.transform:\n","            sample[\"image\"] = self.transform(sample[\"image\"])\n","        return (sample)"]},{"cell_type":"markdown","metadata":{"id":"wXIKKnjnpSgl"},"source":["Data augmentation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzcO7F9A3b8F"},"outputs":[],"source":["imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","tfms = T.Compose([\n","    T.ToPILImage(),\n","    T.RandomCrop((128, 64), padding=8, padding_mode='reflect'),\n","    T.RandomHorizontalFlip(p=0.5), \n","    T.RandomRotation(10),\n","    T.ToTensor(), \n","    T.Normalize(*imagenet_stats,inplace=True), \n","    T.RandomErasing(p=0.5, inplace=True)\n","])\n","\n","valid_tfms = T.Compose([\n","    T.ToTensor(), \n","    T.Normalize(*imagenet_stats)\n","])"]},{"cell_type":"markdown","metadata":{"id":"JhQ0Haapmg92"},"source":["## Dataloaders\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GXufqYPdrTk"},"outputs":[],"source":["def get_data(batch_size):\n","  # Load data\n","  dataset = custom_dataset(\n","      csv_file=annotation_reid_path,\n","      root_dir=TRAIN_FOLDER_PATH,\n","      transform=tfms)\n","      \n","  tr_size = int(len(dataset) * TRAIN_PERCENTAGE/100)\n","  val_size = int(len(dataset)) - tr_size\n","  train, valid=torch.utils.data.random_split(dataset, [tr_size,val_size])\n","\n","  # Initialize dataloaders\n","  train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True)\n","  valid_loader = torch.utils.data.DataLoader(valid, batch_size, shuffle=True)\n","  \n","  return train_loader, valid_loader\n","\n","train_loader, valid_loader = get_data(batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-jN0rdA1q_C"},"outputs":[],"source":["def get_data_test():\n","  query_dataset = custom_dataset_test(\n","      csv_file=QUERY_CSV_PATH,\n","      root_dir=QUERY_FOLDER_PATH,\n","      transform=valid_tfms)\n","      \n","  query_loader = torch.utils.data.DataLoader(query_dataset, 32, shuffle=True)\n","\n","  test_dataset = custom_dataset_test(\n","        csv_file=TEST_CSV_PATH,\n","        root_dir=TEST_FOLDER_PATH,\n","        transform=valid_tfms)\n","        \n","  test_loader = torch.utils.data.DataLoader(test_dataset, 32, shuffle=True)\n","\n","  return query_loader, test_loader\n"]},{"cell_type":"markdown","metadata":{"id":"vOts35p-mqdS"},"source":["## Network definition\n","\n","ResNet50 pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Vm4ueF1lPP8"},"outputs":[],"source":["class ResNet50(nn.Module):\n","  def __init__(self, **kwargs):\n","    super(ResNet50, self).__init__()\n","    # returns a model consisting of all layers of resnet50 but the last one (a fully connected layer), with fixed parameters\n","    resnet50 = models.resnet50(pretrained=True)\n","    modules=list(resnet50.children())[:-1]\n","    self.base=nn.Sequential(*modules)\n","\n","  def forward(self, x):\n","    x = self.base(x)\n","    y = x.view(x.size(0), -1)\n","\n","    return y\n","\n","model = ResNet50().to(device)"]},{"cell_type":"markdown","metadata":{"id":"ie4jCW6vm548"},"source":["## Loss / cost function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWm3vyxXYPzh"},"outputs":[],"source":["class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","        \n","    def calc_euclidean(self, x1, x2):\n","        return (x1 - x2).pow(2).sum(1)\n","    \n","    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n","        distance_positive = self.calc_euclidean(anchor, positive)\n","        distance_negative = self.calc_euclidean(anchor, negative)\n","        losses = torch.relu(distance_positive - distance_negative + self.margin)\n","\n","        return losses.mean()\n","\n","\n","def triplet_cost_function():\n","  cost_function = torch.jit.script(TripletLoss())\n","  return cost_function"]},{"cell_type":"markdown","metadata":{"id":"QaWrHpQim_JK"},"source":["## Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H65Gz5SWZbuI"},"outputs":[],"source":["def get_optimizer(model, lr, wd, momentum):\n","  \n","  #optimizer=torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999),weight_decay=wd, eps=1e-08, amsgrad=False)\n","  optimizer=torch.optim.RMSprop(model.parameters(), lr = lr, alpha = 0.9)\n","  #optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","  #optimizer=torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=wd)\n","  return optimizer\n"]},{"cell_type":"markdown","metadata":{"id":"EGINmaXenDor"},"source":["## Train step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbt8t6peZD3A"},"outputs":[],"source":["def train_model(model,train_loader,criterion,optimizer):\n","  train_loss = 0.0\n","  samples= 0.0\n","  cumulative_train_loss = 0.0\n","  running_loss=[]\n","\n","  model.train()\n","  for batch_idx, sample in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n","\n","      # Load data into GPU\n","      anchor_img = sample['anchor_image'].to(device)\n","      positive_img = sample['positive_image'].to(device)\n","      negative_img= sample['negative_image'].to(device)\n","      anchor_label = sample['label_id'].to(device)\n","\n","      labels=anchor_label.type(torch.LongTensor)\n","                                                                                                 \n","      # Reset the gradients\n","      optimizer.zero_grad()\n","\n","      # Forward pass\n","      anchor_out = model(anchor_img)\n","      positive_out = model(positive_img)\n","      negative_out = model(negative_img)\n","                  \n","      # Apply the loss\n","      loss = criterion(anchor_out, positive_out, negative_out)\n","\n","      # Backward pass\n","      loss.backward()\n","\n","      # Update parameters\n","      optimizer.step()\n","      cumulative_train_loss += loss.item()\n","\n","      running_loss.append(loss.cpu().detach().numpy())\n","\n","      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","\n","  return np.mean(running_loss)"]},{"cell_type":"markdown","metadata":{"id":"vD80YVaGnIh2"},"source":["## Validation step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfTA_lu_ZD7Z"},"outputs":[],"source":["def valid_model(model,valid_loader ,criterion):\n","  valid_loss = 0.0\n","  best=np.inf\n","  samples= 0.0\n","  cumulative_valid_loss = 0.0\n","  valid_results = []\n","  labels_for_prediction = []\n","  running_loss=[]\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for batch_idx, sample in enumerate(tqdm(valid_loader, desc=\"Validation\", leave=False)):\n","\n","        # Load data into GPU\n","        anchor_img = sample['anchor_image'].to(device)\n","        positive_img = sample['positive_image'].to(device)\n","        negative_img= sample['negative_image'].to(device)\n","        anchor_label = sample['label_id'].to(device)\n","        \n","        labels = anchor_label.type(torch.LongTensor)\n","        \n","        valid_results.append(model(anchor_img).cpu().numpy())\n","        labels_for_prediction.append(anchor_label)\n","                                                                                                          \n","        # Forward pass\n","        anchor_out = model(anchor_img)\n","        positive_out = model(positive_img)\n","        negative_out = model(negative_img)\n","                    \n","        # Apply the loss\n","        loss = criterion(anchor_out, positive_out, negative_out)\n","\n","        cumulative_valid_loss += loss.item()\n","\n","        running_loss.append(loss.cpu().detach().numpy())\n","\n","        valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","    \n","\n","  return np.mean(running_loss)"]},{"cell_type":"markdown","metadata":{"id":"f5MAHEGnnc29"},"source":["## Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncchq065nglt"},"outputs":[],"source":["def log_values(writer, step, loss, prefix):\n","  writer.add_scalar(f\"{prefix}/loss\", loss, step)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYj8BP2fZD-s"},"outputs":[],"source":["def main(batch_size=BATCH_SIZE, \n","         device='cuda:0', \n","         learning_rate=0.001, \n","         weight_decay=0.01, \n","         momentum=0.9, \n","         epochs=50, \n","         train_loader=train_loader, \n","         valid_loader=valid_loader, \n","         checkpoint_path=MODEL_PATH):\n","  \n","  # Creates a logger for the experiment\n","  writer = SummaryWriter(log_dir=\"runs/loss\")\n","\n","  early_stopping = EarlyStopping(verbose=True)\n","  \n","  # Instantiates the model\n","  net=ResNet50().to(device)\n","\n","  # Instantiates the optimizer\n","  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","  \n","  # Instantiates the cost function\n","  cost_function = triplet_cost_function()\n","\n","  print('Before training:')\n","  train_loss= train_model(net,train_loader ,cost_function, optimizer)\n","  valid_loss= valid_model(net,valid_loader,cost_function)\n","  \n","\n","  print('\\t Training loss {:.5f}'.format(train_loss))\n","  print('\\t Valid loss {:.5f}'.format(valid_loss))\n","  print('-----------------------------------------------------')\n","    \n","  # Add values to plots\n","  writer.add_scalar('Loss/train_loss', train_loss, 0)\n","  writer.add_scalar('Loss/valid_loss', valid_loss, 0)\n","\n","  for e in tqdm(range(epochs), desc=\"Epochs\"):\n","    train_loss= train_model(net,train_loader ,cost_function, optimizer)\n","    valid_loss= valid_model(net,valid_loader,cost_function)\n","      \n","    print('Epoch: {:d}'.format(e+1))\n","    print('\\t Training loss {:.5f}'.format(train_loss))\n","    print('\\t Valid loss {:.5f}'.format(valid_loss))\n","    \n","     # log to tensorboard\n","    log_values(writer, e, train_loss,\"Train\")\n","    log_values(writer, e, valid_loss, \"Validation\")\n","\n","    early_stopping(valid_loss, net)\n","\n","    if early_stopping.early_stop:\n","      print(\"Early stopping has occurred. Reverting to latest save model\")\n","      break\n","    \n","  # Closes the logger\n","  writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPMd3fsAZEE4"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir=runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8RM8WiMZEB-"},"outputs":[],"source":["main()"]},{"cell_type":"markdown","metadata":{"id":"EHn5G6xvC_A2"},"source":["#mAP Calculation\n","\n","Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg1LwB12DEWU"},"outputs":[],"source":["model = torch.load(MODEL_PATH)"]},{"cell_type":"markdown","metadata":{"id":"kD7YKDOOohhA"},"source":["Validate model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x2rjrG5DEcn"},"outputs":[],"source":["def validate_model(model, loader):\n","  model.eval()\n","\n","  results = []\n","  img_names = []\n","  labels = []\n","  with torch.no_grad():\n","      for step, sample in enumerate(tqdm(loader)):\n","          anchor_img,img_path = sample['anchor_image'].to(device),sample['img_path']\n","          anchor_label = sample['label_id']\n","          results.append(model(anchor_img).cpu().numpy())\n","          img_names.append(img_path)\n","          labels.append(anchor_label)\n","\n","  results = np.concatenate(results)\n","  img_names = np.concatenate(img_names)\n","  labels = np.concatenate(labels)\n","  print(img_names.shape)\n","  print(results.shape)\n","  print(labels.shape)\n","\n","  return results, img_names, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMVEi3nlDEif"},"outputs":[],"source":["val_results, val_img_names, val_labels = validate_model(model, valid_loader)"]},{"cell_type":"markdown","metadata":{"id":"SncLiHKshB9y"},"source":["## Predictions and groundtruth\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ru9FYa_Yaz6-"},"outputs":[],"source":["def predict(validation_results, validation_img_names):\n","  '''\n","    Calculating predictions on the validation set\n","  '''\n","  validation_results_df = pd.DataFrame(validation_results)\n","  validation_results_df['img_path'] = validation_img_names\n","  ids_ = []\n","  for k in validation_results_df['img_path']:\n","    ids_.append(k.split('/')[4].split('_')[0])\n","  validation_results_df['person_id'] = ids_ # val_results is a dataframe where each row contains the image name, the path and all the 20148 features\n","\n","  cosine_df=pd.DataFrame(cosine_similarity(validation_results_df.iloc[:,:-2], validation_results_df.iloc[:,:-2], dense_output=True))\n","  cosine_df=cosine_df.set_index(validation_results_df.img_path)\n","  cosine_df.columns=validation_results_df.img_path # cosine_df is a dataframe thatcontains in each cell the cosine similarity btw image in row with image in column\n","  predictions = {}\n","  threshold = 0.9975\n","\n","  for i in tqdm(range(len(cosine_df))):\n","    tmp_df = pd.DataFrame(cosine_df.iloc[i].sort_values(ascending=False))\n","    id = (tmp_df.columns[0]).split('/')[4].split('_')[0]\n","    images_list = []\n","    for index, row in tmp_df.iterrows():\n","      value = row[0]\n","      if(value < 1.0 and value >= threshold):\n","        images_list.append(index)\n","      if(value < threshold):\n","        break\n","    predictions[id] = images_list\n","  '''\n","    Calculating the groundtruth in order to compute the mAP\n","  '''\n","  gt = {}\n","  for id in ids_:\n","    img_list = []\n","    for img_name in validation_img_names:\n","      if id == img_name.split('/')[4].split('_')[0]:\n","        img_list.append(img_name)\n","    gt[id] = set(img_list)\n","  \n","  return predictions, gt\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0VUzAYHdeSH"},"outputs":[],"source":["predictions_dict, groundtruth_dict = predict(val_results, val_img_names)"]},{"cell_type":"markdown","metadata":{"id":"HiYPb4NSoqKM"},"source":["##mAP result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m9vR1oidssp"},"outputs":[],"source":["Evaluator.evaluate_map(predictions_dict, groundtruth_dict)"]},{"cell_type":"markdown","metadata":{"id":"EFqCNU75TrOx"},"source":["# Submission"]},{"cell_type":"markdown","metadata":{"id":"JZ3_POlXo5OB"},"source":["Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6111EwxqRve"},"outputs":[],"source":["def test_model(model, loader):\n","  model.eval()\n","\n","  results = []\n","  img_names = []\n","  with torch.no_grad():\n","      for step, sample in enumerate(tqdm(loader)):\n","          anchor_img,img_path = sample['image'].to(device),sample['image_path']\n","          results.append(model(anchor_img).cpu().numpy())\n","          img_names.append(img_path)\n","\n","  results = np.concatenate(results)\n","  img_names = np.concatenate(img_names)\n","  print(img_names.shape)\n","  print(results.shape)\n","\n","  return results, img_names"]},{"cell_type":"markdown","metadata":{"id":"J6XgiqDopDtX"},"source":["Create a sumbission dictionary to be converted in a txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBu7EjFsLk0O"},"outputs":[],"source":["def submit():\n","  # Query csv\n","  query_imgs = os.listdir(QUERY_FOLDER_PATH)\n","  query_df = pd.DataFrame(query_imgs, columns=['img_names'])\n","  query_df.to_csv(QUERY_CSV_PATH)\n","  # Test csv\n","  test_imgs = os.listdir(TEST_FOLDER_PATH)\n","  test_df = pd.DataFrame(test_imgs, columns=['img_names'])\n","  test_df.to_csv(TEST_CSV_PATH)\n","\n","  # Get dataloaders for query and test\n","  query_loader, test_loader = get_data_test()\n","\n","  # Make predictions on query and test\n","  q_results, queries_img_names = test_model(model, query_loader)\n","  t_results, test_img_names = test_model(model, test_loader)\n","\n","  # Create a dataframe for both\n","  queries_results=pd.DataFrame(q_results)\n","  queries_results[\"img_name\"]=queries_img_names\n","  test_results=pd.DataFrame(t_results)\n","  test_results[\"img_name\"]=test_img_names\n","\n","  # Cosine similarity dataframe\n","  submission_cos_sim=pd.DataFrame(cosine_similarity(queries_results.iloc[:,:-1], test_results.iloc[:,:-1])) # apply cosine similarity to the 2 vectors of features but the last column\n","  submission_cos_sim=submission_cos_sim.set_index(queries_results.img_name)\n","  submission_cos_sim.columns=test_results.img_name\n","\n","  # Make final predictions for sumbission\n","  submissions = {}\n","  threshold = 0.9975\n","  for i in tqdm(range(len(submission_cos_sim))):\n","    tmp_df = pd.DataFrame(submission_cos_sim.iloc[i].sort_values(ascending=False))\n","    id = (tmp_df.columns[0]).split('/')[4].split('_')[0]\n","    # images list\n","    images_list = []\n","    for index, row in tmp_df.iterrows():\n","      value = row[0]\n","      if(value < 1.0 and value >= threshold):\n","        images_list.append(index.split('/')[4].split('_')[0])\n","      if(value < threshold):\n","        break\n","    submissions[id] = images_list\n","\n","  return submissions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxbUrAhCNRIP"},"outputs":[],"source":["submissions_dict = submit()"]},{"cell_type":"markdown","metadata":{"id":"5-oxllk7pKTg"},"source":["Convert dictionary to txt and save it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"376B8BpjNYSW"},"outputs":[],"source":["l = []\n","for key in submissions_dict.keys():\n","  string = ''\n","  string = string + key + ': '\n","  comma = 0\n","  for elm in submissions_dict[key]:\n","    if (comma == 0):\n","      string = string + elm\n","      comma = 1\n","    if(comma != 0):\n","      string = string + ', ' + elm\n","  l.append(string)\n","f = open('/content/dataset/reid_test.txt', 'w')\n","for st in l:\n","  f.write(st)\n","  f.write('\\n')\n","f.close()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Reidentification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
